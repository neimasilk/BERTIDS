{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Baseline for BERT-IDS\n",
    "\n",
    "This notebook implements a Random Forest baseline model for network intrusion detection using the CICIDS2017 dataset.\n",
    "\n",
    "## Objectives:\n",
    "1. Load and preprocess CICIDS2017 dataset\n",
    "2. Implement comprehensive data preprocessing pipeline\n",
    "3. Train Random Forest classifier\n",
    "4. Evaluate model performance with detailed metrics\n",
    "5. Analyze feature importance\n",
    "6. Establish baseline performance for BERT-IDS comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    roc_curve, precision_recall_curve, average_precision_score\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Additional utilities\n",
    "import joblib\n",
    "from collections import Counter\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Configure settings\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"üìö Libraries imported successfully!\")\n",
    "print(f\"üïê Notebook started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "DATA_DIR = Path('../data/raw/cicids2017')\n",
    "PROCESSED_DIR = Path('../data/processed')\n",
    "MODELS_DIR = Path('../models/checkpoints')\n",
    "RESULTS_DIR = Path('../results')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for dir_path in [PROCESSED_DIR, MODELS_DIR, RESULTS_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Data directory: {DATA_DIR}\")\n",
    "print(f\"üìÅ Processed directory: {PROCESSED_DIR}\")\n",
    "print(f\"üìÅ Models directory: {MODELS_DIR}\")\n",
    "print(f\"üìÅ Results directory: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced data loading function\n",
    "def load_and_preprocess_data(data_dir, sample_size=None, test_mode=True):\n",
    "    \"\"\"\n",
    "    Load and preprocess CICIDS2017 dataset.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Path to data directory\n",
    "        sample_size: Number of samples to load (None for all)\n",
    "        test_mode: If True, use smaller sample for testing\n",
    "    \n",
    "    Returns:\n",
    "        Preprocessed DataFrame\n",
    "    \"\"\"\n",
    "    csv_files = list(data_dir.glob('*.csv'))\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(\"‚ùå No CSV files found! Creating synthetic data for testing...\")\n",
    "        return create_synthetic_data(sample_size or 10000)\n",
    "    \n",
    "    print(f\"üìñ Loading data from {len(csv_files)} files...\")\n",
    "    \n",
    "    dataframes = []\n",
    "    for file in csv_files:\n",
    "        try:\n",
    "            print(f\"   Loading {file.name}...\")\n",
    "            df = pd.read_csv(file, encoding='utf-8', low_memory=False)\n",
    "            \n",
    "            # Clean column names\n",
    "            df.columns = df.columns.str.strip().str.replace(' ', '_')\n",
    "            \n",
    "            dataframes.append(df)\n",
    "            print(f\"   ‚úÖ Loaded {len(df):,} rows\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error loading {file.name}: {e}\")\n",
    "    \n",
    "    if not dataframes:\n",
    "        print(\"‚ùå No data loaded! Creating synthetic data...\")\n",
    "        return create_synthetic_data(sample_size or 10000)\n",
    "    \n",
    "    # Combine dataframes\n",
    "    df = pd.concat(dataframes, ignore_index=True)\n",
    "    print(f\"üîÑ Combined dataset: {len(df):,} rows, {len(df.columns)} columns\")\n",
    "    \n",
    "    # Sample if requested\n",
    "    if sample_size and len(df) > sample_size:\n",
    "        df = df.sample(n=sample_size, random_state=42)\n",
    "        print(f\"üé≤ Sampled {sample_size:,} rows\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_synthetic_data(n_samples=10000):\n",
    "    \"\"\"\n",
    "    Create synthetic network traffic data for testing.\n",
    "    \"\"\"\n",
    "    print(f\"üîß Creating synthetic dataset with {n_samples:,} samples...\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create synthetic features similar to network traffic\n",
    "    data = {\n",
    "        'Flow_Duration': np.random.exponential(1000, n_samples),\n",
    "        'Total_Fwd_Packets': np.random.poisson(10, n_samples),\n",
    "        'Total_Backward_Packets': np.random.poisson(8, n_samples),\n",
    "        'Total_Length_of_Fwd_Packets': np.random.exponential(500, n_samples),\n",
    "        'Total_Length_of_Bwd_Packets': np.random.exponential(400, n_samples),\n",
    "        'Fwd_Packet_Length_Max': np.random.gamma(2, 100, n_samples),\n",
    "        'Fwd_Packet_Length_Min': np.random.gamma(1, 50, n_samples),\n",
    "        'Fwd_Packet_Length_Mean': np.random.normal(200, 50, n_samples),\n",
    "        'Fwd_Packet_Length_Std': np.random.gamma(1, 30, n_samples),\n",
    "        'Bwd_Packet_Length_Max': np.random.gamma(2, 80, n_samples),\n",
    "        'Bwd_Packet_Length_Min': np.random.gamma(1, 40, n_samples),\n",
    "        'Bwd_Packet_Length_Mean': np.random.normal(150, 40, n_samples),\n",
    "        'Bwd_Packet_Length_Std': np.random.gamma(1, 25, n_samples),\n",
    "        'Flow_Bytes/s': np.random.exponential(1000, n_samples),\n",
    "        'Flow_Packets/s': np.random.exponential(10, n_samples),\n",
    "        'Flow_IAT_Mean': np.random.exponential(100, n_samples),\n",
    "        'Flow_IAT_Std': np.random.exponential(50, n_samples),\n",
    "        'Flow_IAT_Max': np.random.exponential(200, n_samples),\n",
    "        'Flow_IAT_Min': np.random.exponential(10, n_samples),\n",
    "        'Fwd_IAT_Total': np.random.exponential(500, n_samples),\n",
    "        'Fwd_IAT_Mean': np.random.exponential(80, n_samples),\n",
    "        'Fwd_IAT_Std': np.random.exponential(40, n_samples),\n",
    "        'Fwd_IAT_Max': np.random.exponential(150, n_samples),\n",
    "        'Fwd_IAT_Min': np.random.exponential(8, n_samples),\n",
    "        'Bwd_IAT_Total': np.random.exponential(400, n_samples),\n",
    "        'Bwd_IAT_Mean': np.random.exponential(70, n_samples),\n",
    "        'Bwd_IAT_Std': np.random.exponential(35, n_samples),\n",
    "        'Bwd_IAT_Max': np.random.exponential(120, n_samples),\n",
    "        'Bwd_IAT_Min': np.random.exponential(6, n_samples),\n",
    "        'Fwd_PSH_Flags': np.random.binomial(5, 0.1, n_samples),\n",
    "        'Bwd_PSH_Flags': np.random.binomial(3, 0.1, n_samples),\n",
    "        'Fwd_URG_Flags': np.random.binomial(1, 0.01, n_samples),\n",
    "        'Bwd_URG_Flags': np.random.binomial(1, 0.01, n_samples),\n",
    "        'Fwd_Header_Length': np.random.normal(20, 5, n_samples),\n",
    "        'Bwd_Header_Length': np.random.normal(20, 5, n_samples),\n",
    "        'Fwd_Packets/s': np.random.exponential(5, n_samples),\n",
    "        'Bwd_Packets/s': np.random.exponential(4, n_samples),\n",
    "        'Min_Packet_Length': np.random.gamma(1, 20, n_samples),\n",
    "        'Max_Packet_Length': np.random.gamma(3, 200, n_samples),\n",
    "        'Packet_Length_Mean': np.random.normal(180, 60, n_samples),\n",
    "        'Packet_Length_Std': np.random.gamma(2, 40, n_samples),\n",
    "        'Packet_Length_Variance': np.random.gamma(3, 500, n_samples),\n",
    "        'FIN_Flag_Count': np.random.binomial(2, 0.3, n_samples),\n",
    "        'SYN_Flag_Count': np.random.binomial(2, 0.2, n_samples),\n",
    "        'RST_Flag_Count': np.random.binomial(1, 0.1, n_samples),\n",
    "        'PSH_Flag_Count': np.random.binomial(3, 0.15, n_samples),\n",
    "        'ACK_Flag_Count': np.random.binomial(10, 0.8, n_samples),\n",
    "        'URG_Flag_Count': np.random.binomial(1, 0.01, n_samples),\n",
    "        'CWE_Flag_Count': np.random.binomial(1, 0.005, n_samples),\n",
    "        'ECE_Flag_Count': np.random.binomial(1, 0.005, n_samples),\n",
    "        'Down/Up_Ratio': np.random.gamma(1, 1, n_samples),\n",
    "        'Average_Packet_Size': np.random.normal(200, 80, n_samples),\n",
    "        'Avg_Fwd_Segment_Size': np.random.normal(180, 70, n_samples),\n",
    "        'Avg_Bwd_Segment_Size': np.random.normal(160, 60, n_samples),\n",
    "        'Fwd_Header_Length.1': np.random.normal(20, 5, n_samples),\n",
    "        'Fwd_Avg_Bytes/Bulk': np.random.exponential(100, n_samples),\n",
    "        'Fwd_Avg_Packets/Bulk': np.random.exponential(5, n_samples),\n",
    "        'Fwd_Avg_Bulk_Rate': np.random.exponential(50, n_samples),\n",
    "        'Bwd_Avg_Bytes/Bulk': np.random.exponential(80, n_samples),\n",
    "        'Bwd_Avg_Packets/Bulk': np.random.exponential(4, n_samples),\n",
    "        'Bwd_Avg_Bulk_Rate': np.random.exponential(40, n_samples),\n",
    "        'Subflow_Fwd_Packets': np.random.poisson(8, n_samples),\n",
    "        'Subflow_Fwd_Bytes': np.random.exponential(400, n_samples),\n",
    "        'Subflow_Bwd_Packets': np.random.poisson(6, n_samples),\n",
    "        'Subflow_Bwd_Bytes': np.random.exponential(300, n_samples),\n",
    "        'Init_Win_bytes_forward': np.random.normal(8192, 2000, n_samples),\n",
    "        'Init_Win_bytes_backward': np.random.normal(8192, 2000, n_samples),\n",
    "        'act_data_pkt_fwd': np.random.poisson(5, n_samples),\n",
    "        'min_seg_size_forward': np.random.gamma(1, 20, n_samples),\n",
    "        'Active_Mean': np.random.exponential(1000, n_samples),\n",
    "        'Active_Std': np.random.exponential(500, n_samples),\n",
    "        'Active_Max': np.random.exponential(2000, n_samples),\n",
    "        'Active_Min': np.random.exponential(100, n_samples),\n",
    "        'Idle_Mean': np.random.exponential(5000, n_samples),\n",
    "        'Idle_Std': np.random.exponential(2000, n_samples),\n",
    "        'Idle_Max': np.random.exponential(10000, n_samples),\n",
    "        'Idle_Min': np.random.exponential(500, n_samples)\n",
    "    }\n",
    "    \n",
    "    # Create labels with realistic distribution\n",
    "    # 80% normal traffic, 20% attacks\n",
    "    attack_types = ['BENIGN', 'DoS_Hulk', 'PortScan', 'DDoS', 'DoS_GoldenEye', 'FTP-Patator', 'SSH-Patator', 'DoS_slowloris', 'DoS_Slowhttptest', 'Bot', 'Web_Attack_Brute_Force', 'Web_Attack_XSS', 'Infiltration', 'Web_Attack_Sql_Injection', 'Heartbleed']\n",
    "    \n",
    "    # Weighted probabilities (BENIGN is most common)\n",
    "    probabilities = [0.8] + [0.2/14] * 14  # 80% benign, 20% attacks distributed\n",
    "    labels = np.random.choice(attack_types, n_samples, p=probabilities)\n",
    "    \n",
    "    data['Label'] = labels\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Add some realistic correlations and noise\n",
    "    # Make attack traffic slightly different\n",
    "    attack_mask = df['Label'] != 'BENIGN'\n",
    "    df.loc[attack_mask, 'Flow_Duration'] *= np.random.uniform(0.5, 2.0, attack_mask.sum())\n",
    "    df.loc[attack_mask, 'Total_Fwd_Packets'] *= np.random.uniform(1.2, 3.0, attack_mask.sum())\n",
    "    df.loc[attack_mask, 'Flow_Bytes/s'] *= np.random.uniform(0.3, 1.5, attack_mask.sum())\n",
    "    \n",
    "    print(f\"‚úÖ Created synthetic dataset: {len(df):,} rows, {len(df.columns)} columns\")\n",
    "    print(f\"üìä Label distribution:\")\n",
    "    label_counts = df['Label'].value_counts()\n",
    "    for label, count in label_counts.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"   {label:<25}: {count:>6,} ({percentage:>5.1f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load data\n",
    "print(\"üìä Loading CICIDS2017 dataset...\")\n",
    "df = load_and_preprocess_data(DATA_DIR, sample_size=50000)  # Use 50k samples for faster processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive preprocessing pipeline\n",
    "class NetworkTrafficPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.scaler = RobustScaler()  # Robust to outliers\n",
    "        self.imputer = SimpleImputer(strategy='median')\n",
    "        self.feature_selector = None\n",
    "        self.feature_names = None\n",
    "        self.label_mapping = None\n",
    "        \n",
    "    def fit_transform(self, df, target_col='Label', n_features=None):\n",
    "        \"\"\"\n",
    "        Fit preprocessor and transform data.\n",
    "        \"\"\"\n",
    "        print(\"üîß Starting preprocessing pipeline...\")\n",
    "        \n",
    "        # Separate features and target\n",
    "        X = df.drop(columns=[target_col])\n",
    "        y = df[target_col]\n",
    "        \n",
    "        print(f\"   Original shape: {X.shape}\")\n",
    "        \n",
    "        # 1. Handle infinite values\n",
    "        print(\"   üßπ Handling infinite values...\")\n",
    "        X = X.replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        # 2. Remove constant features\n",
    "        print(\"   üßπ Removing constant features...\")\n",
    "        constant_features = [col for col in X.columns if X[col].nunique() <= 1]\n",
    "        if constant_features:\n",
    "            print(f\"      Removing {len(constant_features)} constant features\")\n",
    "            X = X.drop(columns=constant_features)\n",
    "        \n",
    "        # 3. Handle missing values\n",
    "        print(\"   üßπ Imputing missing values...\")\n",
    "        X_imputed = self.imputer.fit_transform(X)\n",
    "        X = pd.DataFrame(X_imputed, columns=X.columns, index=X.index)\n",
    "        \n",
    "        # 4. Feature selection (optional)\n",
    "        if n_features and n_features < X.shape[1]:\n",
    "            print(f\"   üéØ Selecting top {n_features} features...\")\n",
    "            # Encode labels temporarily for feature selection\n",
    "            y_encoded = self.label_encoder.fit_transform(y)\n",
    "            \n",
    "            self.feature_selector = SelectKBest(score_func=f_classif, k=n_features)\n",
    "            X_selected = self.feature_selector.fit_transform(X, y_encoded)\n",
    "            \n",
    "            # Get selected feature names\n",
    "            selected_features = X.columns[self.feature_selector.get_support()]\n",
    "            X = pd.DataFrame(X_selected, columns=selected_features, index=X.index)\n",
    "            print(f\"      Selected features: {list(selected_features[:10])}...\")\n",
    "        \n",
    "        # 5. Scale features\n",
    "        print(\"   ‚öñÔ∏è  Scaling features...\")\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        X = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "        \n",
    "        # 6. Encode labels\n",
    "        print(\"   üè∑Ô∏è  Encoding labels...\")\n",
    "        y_encoded = self.label_encoder.fit_transform(y)\n",
    "        \n",
    "        # Store feature names and label mapping\n",
    "        self.feature_names = X.columns.tolist()\n",
    "        self.label_mapping = dict(zip(self.label_encoder.classes_, \n",
    "                                    self.label_encoder.transform(self.label_encoder.classes_)))\n",
    "        \n",
    "        print(f\"   ‚úÖ Final shape: {X.shape}\")\n",
    "        print(f\"   üìä Label mapping: {self.label_mapping}\")\n",
    "        \n",
    "        return X, y_encoded\n",
    "    \n",
    "    def transform(self, df, target_col='Label'):\n",
    "        \"\"\"\n",
    "        Transform new data using fitted preprocessor.\n",
    "        \"\"\"\n",
    "        X = df.drop(columns=[target_col])\n",
    "        y = df[target_col]\n",
    "        \n",
    "        # Apply same transformations\n",
    "        X = X.replace([np.inf, -np.inf], np.nan)\n",
    "        X_imputed = self.imputer.transform(X)\n",
    "        X = pd.DataFrame(X_imputed, columns=X.columns, index=X.index)\n",
    "        \n",
    "        if self.feature_selector:\n",
    "            X_selected = self.feature_selector.transform(X)\n",
    "            X = pd.DataFrame(X_selected, columns=self.feature_names, index=X.index)\n",
    "        \n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        X = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "        \n",
    "        y_encoded = self.label_encoder.transform(y)\n",
    "        \n",
    "        return X, y_encoded\n",
    "\n",
    "# Initialize and apply preprocessor\n",
    "preprocessor = NetworkTrafficPreprocessor()\n",
    "X_processed, y_processed = preprocessor.fit_transform(df, n_features=50)  # Select top 50 features\n",
    "\n",
    "print(f\"\\nüìä Preprocessing Summary:\")\n",
    "print(f\"   Features: {X_processed.shape[1]}\")\n",
    "print(f\"   Samples: {X_processed.shape[0]}\")\n",
    "print(f\"   Classes: {len(np.unique(y_processed))}\")\n",
    "print(f\"   Feature names: {preprocessor.feature_names[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train-Test Split and Class Balance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data with stratification\n",
    "print(\"üîÑ Splitting data into train/validation/test sets...\")\n",
    "\n",
    "# First split: train+val vs test (80-20)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_processed, y_processed, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_processed\n",
    ")\n",
    "\n",
    "# Second split: train vs val (75-25 of remaining 80%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.25, \n",
    "    random_state=42, \n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"üìä Data split summary:\")\n",
    "print(f\"   Training set:   {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X_processed)*100:.1f}%)\")\n",
    "print(f\"   Validation set: {X_val.shape[0]:,} samples ({X_val.shape[0]/len(X_processed)*100:.1f}%)\")\n",
    "print(f\"   Test set:       {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X_processed)*100:.1f}%)\")\n",
    "\n",
    "# Analyze class distribution\n",
    "def analyze_class_distribution(y, set_name, label_encoder):\n",
    "    \"\"\"\n",
    "    Analyze and display class distribution.\n",
    "    \"\"\"\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    class_names = label_encoder.inverse_transform(unique)\n",
    "    \n",
    "    print(f\"\\nüìä {set_name} class distribution:\")\n",
    "    for class_name, count in zip(class_names, counts):\n",
    "        percentage = (count / len(y)) * 100\n",
    "        print(f\"   {class_name:<25}: {count:>6,} ({percentage:>5.1f}%)\")\n",
    "    \n",
    "    return dict(zip(class_names, counts))\n",
    "\n",
    "train_dist = analyze_class_distribution(y_train, \"Training\", preprocessor.label_encoder)\n",
    "val_dist = analyze_class_distribution(y_val, \"Validation\", preprocessor.label_encoder)\n",
    "test_dist = analyze_class_distribution(y_test, \"Test\", preprocessor.label_encoder)\n",
    "\n",
    "# Calculate class weights for imbalanced data\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "print(f\"\\n‚öñÔ∏è  Class weights for balancing:\")\n",
    "for class_idx, weight in class_weight_dict.items():\n",
    "    class_name = preprocessor.label_encoder.inverse_transform([class_idx])[0]\n",
    "    print(f\"   {class_name:<25}: {weight:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Random Forest Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Random Forest with optimal parameters\n",
    "print(\"üå≤ Configuring Random Forest classifier...\")\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': 100,          # Number of trees\n",
    "    'max_depth': 20,              # Maximum depth of trees\n",
    "    'min_samples_split': 5,       # Minimum samples to split\n",
    "    'min_samples_leaf': 2,        # Minimum samples in leaf\n",
    "    'max_features': 'sqrt',       # Number of features per tree\n",
    "    'bootstrap': True,            # Bootstrap sampling\n",
    "    'class_weight': class_weight_dict,  # Handle class imbalance\n",
    "    'random_state': 42,           # Reproducibility\n",
    "    'n_jobs': -1,                 # Use all CPU cores\n",
    "    'verbose': 1                  # Show progress\n",
    "}\n",
    "\n",
    "print(f\"üìã Random Forest parameters:\")\n",
    "for param, value in rf_params.items():\n",
    "    if param != 'class_weight':\n",
    "        print(f\"   {param:<20}: {value}\")\n",
    "    else:\n",
    "        print(f\"   {param:<20}: balanced\")\n",
    "\n",
    "# Initialize model\n",
    "rf_model = RandomForestClassifier(**rf_params)\n",
    "\n",
    "# Train model\n",
    "print(f\"\\nüöÄ Training Random Forest model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"‚úÖ Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Model information\n",
    "print(f\"\\nüìä Model Information:\")\n",
    "print(f\"   Number of trees: {rf_model.n_estimators}\")\n",
    "print(f\"   Number of features: {rf_model.n_features_in_}\")\n",
    "print(f\"   Number of classes: {rf_model.n_classes_}\")\n",
    "print(f\"   Feature importance available: {hasattr(rf_model, 'feature_importances_')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive model evaluation\n",
    "def evaluate_model(model, X_train, y_train, X_val, y_val, X_test, y_test, label_encoder):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation with multiple metrics.\n",
    "    \"\"\"\n",
    "    print(\"üìä Evaluating model performance...\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Predictions for all sets\n",
    "    sets = {\n",
    "        'Training': (X_train, y_train),\n",
    "        'Validation': (X_val, y_val),\n",
    "        'Test': (X_test, y_test)\n",
    "    }\n",
    "    \n",
    "    for set_name, (X, y) in sets.items():\n",
    "        print(f\"\\nüîç {set_name} Set Evaluation:\")\n",
    "        \n",
    "        # Predictions\n",
    "        start_time = time.time()\n",
    "        y_pred = model.predict(X)\n",
    "        y_pred_proba = model.predict_proba(X)\n",
    "        prediction_time = time.time() - start_time\n",
    "        \n",
    "        # Basic metrics\n",
    "        accuracy = accuracy_score(y, y_pred)\n",
    "        precision = precision_score(y, y_pred, average='weighted', zero_division=0)\n",
    "        recall = recall_score(y, y_pred, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y, y_pred, average='weighted', zero_division=0)\n",
    "        \n",
    "        # Multi-class ROC AUC\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y, y_pred_proba, multi_class='ovr', average='weighted')\n",
    "        except:\n",
    "            roc_auc = 0.0\n",
    "        \n",
    "        print(f\"   Accuracy:  {accuracy:.4f}\")\n",
    "        print(f\"   Precision: {precision:.4f}\")\n",
    "        print(f\"   Recall:    {recall:.4f}\")\n",
    "        print(f\"   F1-Score:  {f1:.4f}\")\n",
    "        print(f\"   ROC AUC:   {roc_auc:.4f}\")\n",
    "        print(f\"   Prediction time: {prediction_time:.4f}s ({len(X)/prediction_time:.0f} samples/s)\")\n",
    "        \n",
    "        # Store results\n",
    "        results[set_name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'roc_auc': roc_auc,\n",
    "            'prediction_time': prediction_time,\n",
    "            'y_true': y,\n",
    "            'y_pred': y_pred,\n",
    "            'y_pred_proba': y_pred_proba\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate model\n",
    "evaluation_results = evaluate_model(\n",
    "    rf_model, X_train, y_train, X_val, y_val, X_test, y_test, preprocessor.label_encoder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report for test set\n",
    "print(\"üìã Detailed Classification Report (Test Set):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "y_test_pred = evaluation_results['Test']['y_pred']\n",
    "class_names = preprocessor.label_encoder.classes_\n",
    "\n",
    "report = classification_report(\n",
    "    y_test, y_test_pred, \n",
    "    target_names=class_names,\n",
    "    digits=4,\n",
    "    zero_division=0\n",
    ")\n",
    "print(report)\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nüîç Confusion Matrix (Test Set):\")\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix - Random Forest (Test Set)')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Per-class accuracy\n",
    "print(\"\\nüéØ Per-Class Accuracy:\")\n",
    "class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "for i, (class_name, acc) in enumerate(zip(class_names, class_accuracy)):\n",
    "    support = cm.sum(axis=1)[i]\n",
    "    print(f\"   {class_name:<25}: {acc:.4f} (support: {support:,})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "print(\"üîç Analyzing feature importance...\")\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance = rf_model.feature_importances_\n",
    "feature_names = preprocessor.feature_names\n",
    "\n",
    "# Create feature importance DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nüìä Top 20 Most Important Features:\")\n",
    "print(\"=\" * 60)\n",
    "for i, (_, row) in enumerate(importance_df.head(20).iterrows(), 1):\n",
    "    print(f\"{i:2d}. {row['feature']:<35}: {row['importance']:.6f}\")\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = importance_df.head(20)\n",
    "sns.barplot(data=top_features, x='importance', y='feature', palette='viridis')\n",
    "plt.title('Top 20 Feature Importances - Random Forest')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cumulative importance\n",
    "cumulative_importance = np.cumsum(importance_df['importance'])\n",
    "n_features_90 = np.argmax(cumulative_importance >= 0.9) + 1\n",
    "n_features_95 = np.argmax(cumulative_importance >= 0.95) + 1\n",
    "\n",
    "print(f\"\\nüìà Cumulative Feature Importance:\")\n",
    "print(f\"   Features for 90% importance: {n_features_90}\")\n",
    "print(f\"   Features for 95% importance: {n_features_95}\")\n",
    "print(f\"   Total features used: {len(feature_names)}\")\n",
    "\n",
    "# Plot cumulative importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(cumulative_importance) + 1), cumulative_importance, 'b-', linewidth=2)\n",
    "plt.axhline(y=0.9, color='r', linestyle='--', alpha=0.7, label='90% threshold')\n",
    "plt.axhline(y=0.95, color='orange', linestyle='--', alpha=0.7, label='95% threshold')\n",
    "plt.axvline(x=n_features_90, color='r', linestyle=':', alpha=0.7)\n",
    "plt.axvline(x=n_features_95, color='orange', linestyle=':', alpha=0.7)\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Cumulative Importance')\n",
    "plt.title('Cumulative Feature Importance')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cross-Validation and Model Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation for model robustness\n",
    "print(\"üîÑ Performing cross-validation...\")\n",
    "\n",
    "# Use stratified k-fold for imbalanced data\n",
    "cv_folds = 5\n",
    "skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Combine train and validation sets for CV\n",
    "X_cv = pd.concat([X_train, X_val], ignore_index=True)\n",
    "y_cv = np.concatenate([y_train, y_val])\n",
    "\n",
    "print(f\"   Using {cv_folds}-fold stratified cross-validation\")\n",
    "print(f\"   CV dataset size: {len(X_cv):,} samples\")\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_start_time = time.time()\n",
    "\n",
    "cv_scores = cross_val_score(\n",
    "    rf_model, X_cv, y_cv, \n",
    "    cv=skf, \n",
    "    scoring='f1_weighted',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "cv_time = time.time() - cv_start_time\n",
    "\n",
    "print(f\"\\nüìä Cross-Validation Results:\")\n",
    "print(f\"   Mean F1-Score: {cv_scores.mean():.4f} (¬±{cv_scores.std()*2:.4f})\")\n",
    "print(f\"   Individual scores: {[f'{score:.4f}' for score in cv_scores]}\")\n",
    "print(f\"   CV time: {cv_time:.2f} seconds\")\n",
    "\n",
    "# Additional CV metrics\n",
    "cv_metrics = ['accuracy', 'precision_weighted', 'recall_weighted']\n",
    "cv_results = {}\n",
    "\n",
    "for metric in cv_metrics:\n",
    "    scores = cross_val_score(rf_model, X_cv, y_cv, cv=skf, scoring=metric, n_jobs=-1)\n",
    "    cv_results[metric] = {\n",
    "        'mean': scores.mean(),\n",
    "        'std': scores.std(),\n",
    "        'scores': scores\n",
    "    }\n",
    "    print(f\"   {metric:<20}: {scores.mean():.4f} (¬±{scores.std()*2:.4f})\")\n",
    "\n",
    "# Plot CV results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "metrics_to_plot = ['f1_weighted'] + cv_metrics\n",
    "all_scores = [cv_scores] + [cv_results[m]['scores'] for m in cv_metrics]\n",
    "\n",
    "for i, (metric, scores) in enumerate(zip(metrics_to_plot, all_scores)):\n",
    "    axes[i].boxplot(scores, labels=[metric.replace('_', ' ').title()])\n",
    "    axes[i].set_title(f'{metric.replace(\"_\", \" \").title()} Distribution')\n",
    "    axes[i].set_ylabel('Score')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add mean line\n",
    "    axes[i].axhline(y=scores.mean(), color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Persistence and Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and preprocessor\n",
    "print(\"üíæ Saving model and preprocessor...\")\n",
    "\n",
    "# Create timestamp for versioning\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Save model\n",
    "model_path = MODELS_DIR / f'random_forest_baseline_{timestamp}.joblib'\n",
    "joblib.dump(rf_model, model_path)\n",
    "print(f\"   ‚úÖ Model saved to: {model_path}\")\n",
    "\n",
    "# Save preprocessor\n",
    "preprocessor_path = MODELS_DIR / f'preprocessor_{timestamp}.joblib'\n",
    "joblib.dump(preprocessor, preprocessor_path)\n",
    "print(f\"   ‚úÖ Preprocessor saved to: {preprocessor_path}\")\n",
    "\n",
    "# Save feature importance\n",
    "importance_path = RESULTS_DIR / f'feature_importance_{timestamp}.csv'\n",
    "importance_df.to_csv(importance_path, index=False)\n",
    "print(f\"   ‚úÖ Feature importance saved to: {importance_path}\")\n",
    "\n",
    "# Comprehensive results summary\n",
    "results_summary = {\n",
    "    'model_info': {\n",
    "        'model_type': 'RandomForestClassifier',\n",
    "        'timestamp': timestamp,\n",
    "        'training_time': training_time,\n",
    "        'n_estimators': rf_model.n_estimators,\n",
    "        'max_depth': rf_model.max_depth,\n",
    "        'n_features': rf_model.n_features_in_,\n",
    "        'n_classes': rf_model.n_classes_\n",
    "    },\n",
    "    'data_info': {\n",
    "        'total_samples': len(df),\n",
    "        'train_samples': len(X_train),\n",
    "        'val_samples': len(X_val),\n",
    "        'test_samples': len(X_test),\n",
    "        'n_features_selected': len(preprocessor.feature_names),\n",
    "        'class_distribution': train_dist\n",
    "    },\n",
    "    'performance': {\n",
    "        'test_accuracy': evaluation_results['Test']['accuracy'],\n",
    "        'test_precision': evaluation_results['Test']['precision'],\n",
    "        'test_recall': evaluation_results['Test']['recall'],\n",
    "        'test_f1_score': evaluation_results['Test']['f1_score'],\n",
    "        'test_roc_auc': evaluation_results['Test']['roc_auc'],\n",
    "        'cv_f1_mean': cv_scores.mean(),\n",
    "        'cv_f1_std': cv_scores.std()\n",
    "    },\n",
    "    'feature_analysis': {\n",
    "        'top_10_features': importance_df.head(10)['feature'].tolist(),\n",
    "        'features_for_90_percent': n_features_90,\n",
    "        'features_for_95_percent': n_features_95\n",
    "    },\n",
    "    'files': {\n",
    "        'model_path': str(model_path),\n",
    "        'preprocessor_path': str(preprocessor_path),\n",
    "        'importance_path': str(importance_path)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save results summary\n",
    "summary_path = RESULTS_DIR / f'baseline_results_summary_{timestamp}.json'\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2, default=str)\n",
    "print(f\"   ‚úÖ Results summary saved to: {summary_path}\")\n",
    "\n",
    "# Display final summary\n",
    "print(f\"\\nüéâ Random Forest Baseline Training Complete!\")\n",
    "print(f\"=\" * 60)\n",
    "print(f\"üìä Final Performance Summary:\")\n",
    "print(f\"   Test Accuracy:  {evaluation_results['Test']['accuracy']:.4f}\")\n",
    "print(f\"   Test Precision: {evaluation_results['Test']['precision']:.4f}\")\n",
    "print(f\"   Test Recall:    {evaluation_results['Test']['recall']:.4f}\")\n",
    "print(f\"   Test F1-Score:  {evaluation_results['Test']['f1_score']:.4f}\")\n",
    "print(f\"   Test ROC AUC:   {evaluation_results['Test']['roc_auc']:.4f}\")\n",
    "print(f\"   CV F1-Score:    {cv_scores.mean():.4f} (¬±{cv_scores.std()*2:.4f})\")\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Training Time: {training_time:.2f} seconds\")\n",
    "print(f\"üîç Cross-Validation Time: {cv_time:.2f} seconds\")\n",
    "print(f\"üöÄ Prediction Speed: {len(X_test)/evaluation_results['Test']['prediction_time']:.0f} samples/second\")\n",
    "\n",
    "print(f\"\\nüìÅ Saved Files:\")\n",
    "print(f\"   Model: {model_path.name}\")\n",
    "print(f\"   Preprocessor: {preprocessor_path.name}\")\n",
    "print(f\"   Feature Importance: {importance_path.name}\")\n",
    "print(f\"   Results Summary: {summary_path.name}\")\n",
    "\n",
    "print(f\"\\nüéØ Next Steps:\")\n",
    "print(f\"   1. Compare with BERT-IDS model performance\")\n",
    "print(f\"   2. Analyze misclassified samples\")\n",
    "print(f\"   3. Experiment with feature engineering\")\n",
    "print(f\"   4. Try ensemble methods\")\n",
    "print(f\"   5. Implement real-time inference pipeline\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert-ids",
   "language": "python",
   "name": "bert-ids"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}