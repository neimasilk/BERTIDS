{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CICIDS2017 Dataset Exploration\n",
    "\n",
    "This notebook provides comprehensive exploration and analysis of the CICIDS2017 dataset for the BERT-IDS research project.\n",
    "\n",
    "## Objectives:\n",
    "1. Load and examine the dataset structure\n",
    "2. Analyze feature distributions and statistics\n",
    "3. Explore attack types and class imbalance\n",
    "4. Identify data quality issues\n",
    "5. Visualize key patterns and relationships\n",
    "6. Prepare insights for tokenization strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"üìö Libraries imported successfully!\")\n",
    "print(f\"üìä Pandas version: {pd.__version__}\")\n",
    "print(f\"üî¢ NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Loading and Initial Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "DATA_DIR = Path('../data/raw/cicids2017')\n",
    "PROCESSED_DIR = Path('../data/processed')\n",
    "\n",
    "# Create processed directory if it doesn't exist\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# List available CSV files\n",
    "csv_files = list(DATA_DIR.glob('*.csv'))\n",
    "print(f\"üìÅ Found {len(csv_files)} CSV files:\")\n",
    "for file in csv_files:\n",
    "    print(f\"   - {file.name}\")\n",
    "\n",
    "if not csv_files:\n",
    "    print(\"‚ö†Ô∏è  No CSV files found!\")\n",
    "    print(\"üìã Please download CICIDS2017 dataset files to: data/raw/cicids2017/\")\n",
    "    print(\"üîó Download from: https://www.unb.ca/cic/datasets/ids-2017.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and combine all CSV files\n",
    "def load_cicids2017_data(data_dir, sample_size=None):\n",
    "    \"\"\"\n",
    "    Load CICIDS2017 dataset from multiple CSV files.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Path to directory containing CSV files\n",
    "        sample_size: Number of samples to load (None for all)\n",
    "    \n",
    "    Returns:\n",
    "        Combined DataFrame\n",
    "    \"\"\"\n",
    "    csv_files = list(data_dir.glob('*.csv'))\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(\"‚ùå No CSV files found!\")\n",
    "        return None\n",
    "    \n",
    "    dataframes = []\n",
    "    total_rows = 0\n",
    "    \n",
    "    for file in csv_files:\n",
    "        print(f\"üìñ Loading {file.name}...\")\n",
    "        try:\n",
    "            # Load with error handling\n",
    "            df = pd.read_csv(file, encoding='utf-8', low_memory=False)\n",
    "            \n",
    "            # Clean column names (remove spaces, special characters)\n",
    "            df.columns = df.columns.str.strip().str.replace(' ', '_')\n",
    "            \n",
    "            print(f\"   ‚úÖ Loaded {len(df):,} rows, {len(df.columns)} columns\")\n",
    "            \n",
    "            dataframes.append(df)\n",
    "            total_rows += len(df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error loading {file.name}: {e}\")\n",
    "    \n",
    "    if not dataframes:\n",
    "        return None\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    print(f\"üîÑ Combining {len(dataframes)} files...\")\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    # Sample if requested\n",
    "    if sample_size and len(combined_df) > sample_size:\n",
    "        print(f\"üé≤ Sampling {sample_size:,} rows from {len(combined_df):,} total rows\")\n",
    "        combined_df = combined_df.sample(n=sample_size, random_state=42)\n",
    "    \n",
    "    print(f\"‚úÖ Final dataset: {len(combined_df):,} rows, {len(combined_df.columns)} columns\")\n",
    "    return combined_df\n",
    "\n",
    "# Load dataset (sample for initial exploration)\n",
    "# For full analysis, set sample_size=None\n",
    "df = load_cicids2017_data(DATA_DIR, sample_size=100000)  # Sample 100k rows for faster exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "if df is not None:\n",
    "    print(\"üìä Dataset Overview:\")\n",
    "    print(f\"   Shape: {df.shape}\")\n",
    "    print(f\"   Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    print(f\"   Data types: {df.dtypes.value_counts().to_dict()}\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(\"\\nüîç First 5 rows:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    # Display column names\n",
    "    print(f\"\\nüìã Column names ({len(df.columns)} total):\")\n",
    "    for i, col in enumerate(df.columns, 1):\n",
    "        print(f\"   {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify label column (usually 'Label' or similar)\n",
    "label_columns = [col for col in df.columns if 'label' in col.lower()]\n",
    "print(f\"üè∑Ô∏è  Potential label columns: {label_columns}\")\n",
    "\n",
    "if label_columns:\n",
    "    label_col = label_columns[0]  # Use first label column\n",
    "    print(f\"üìä Using '{label_col}' as target variable\")\n",
    "    \n",
    "    # Analyze class distribution\n",
    "    class_counts = df[label_col].value_counts()\n",
    "    class_percentages = df[label_col].value_counts(normalize=True) * 100\n",
    "    \n",
    "    print(f\"\\nüéØ Class Distribution:\")\n",
    "    for class_name, count in class_counts.items():\n",
    "        percentage = class_percentages[class_name]\n",
    "        print(f\"   {class_name:<25}: {count:>8,} ({percentage:>5.2f}%)\")\n",
    "    \n",
    "    # Visualize class distribution\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Bar plot\n",
    "    class_counts.plot(kind='bar', ax=ax1, color='skyblue')\n",
    "    ax1.set_title('Class Distribution (Counts)')\n",
    "    ax1.set_xlabel('Attack Type')\n",
    "    ax1.set_ylabel('Count')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Pie chart\n",
    "    ax2.pie(class_counts.values, labels=class_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "    ax2.set_title('Class Distribution (Percentages)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No label column found!\")\n",
    "    label_col = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features from target\n",
    "if label_col:\n",
    "    feature_cols = [col for col in df.columns if col != label_col]\n",
    "    X = df[feature_cols]\n",
    "    y = df[label_col]\n",
    "    \n",
    "    print(f\"üìä Feature Analysis:\")\n",
    "    print(f\"   Number of features: {len(feature_cols)}\")\n",
    "    print(f\"   Numeric features: {X.select_dtypes(include=[np.number]).shape[1]}\")\n",
    "    print(f\"   Categorical features: {X.select_dtypes(include=['object']).shape[1]}\")\n",
    "    \n",
    "    # Data quality check\n",
    "    print(f\"\\nüîç Data Quality:\")\n",
    "    missing_data = X.isnull().sum()\n",
    "    missing_percentage = (missing_data / len(X)) * 100\n",
    "    \n",
    "    quality_df = pd.DataFrame({\n",
    "        'Missing_Count': missing_data,\n",
    "        'Missing_Percentage': missing_percentage,\n",
    "        'Data_Type': X.dtypes\n",
    "    })\n",
    "    \n",
    "    # Show features with missing values\n",
    "    missing_features = quality_df[quality_df['Missing_Count'] > 0]\n",
    "    if len(missing_features) > 0:\n",
    "        print(f\"   Features with missing values: {len(missing_features)}\")\n",
    "        display(missing_features.head(10))\n",
    "    else:\n",
    "        print(\"   ‚úÖ No missing values found!\")\n",
    "    \n",
    "    # Check for infinite values\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "    inf_counts = {}\n",
    "    for col in numeric_cols:\n",
    "        inf_count = np.isinf(X[col]).sum()\n",
    "        if inf_count > 0:\n",
    "            inf_counts[col] = inf_count\n",
    "    \n",
    "    if inf_counts:\n",
    "        print(f\"\\n‚ö†Ô∏è  Features with infinite values:\")\n",
    "        for col, count in inf_counts.items():\n",
    "            print(f\"   {col}: {count} infinite values\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No infinite values found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of numeric features\n",
    "if len(numeric_cols) > 0:\n",
    "    print(\"üìà Statistical Summary (Top 10 Numeric Features):\")\n",
    "    stats_summary = X[numeric_cols].describe()\n",
    "    display(stats_summary.iloc[:, :10])  # Show first 10 features\n",
    "    \n",
    "    # Identify features with zero variance\n",
    "    zero_var_features = [col for col in numeric_cols if X[col].var() == 0]\n",
    "    if zero_var_features:\n",
    "        print(f\"\\n‚ö†Ô∏è  Features with zero variance: {len(zero_var_features)}\")\n",
    "        print(f\"   {zero_var_features[:10]}...\")  # Show first 10\n",
    "    else:\n",
    "        print(\"\\n‚úÖ All numeric features have non-zero variance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Distributions and Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top features for visualization (by variance)\n",
    "if len(numeric_cols) > 0:\n",
    "    # Calculate variance for each numeric feature\n",
    "    feature_variance = X[numeric_cols].var().sort_values(ascending=False)\n",
    "    top_features = feature_variance.head(12).index.tolist()\n",
    "    \n",
    "    print(f\"üìä Top 12 features by variance:\")\n",
    "    for i, feature in enumerate(top_features, 1):\n",
    "        print(f\"   {i:2d}. {feature:<30} (var: {feature_variance[feature]:.2e})\")\n",
    "    \n",
    "    # Plot distributions of top features\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, feature in enumerate(top_features):\n",
    "        # Handle infinite values for plotting\n",
    "        data = X[feature].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        \n",
    "        if len(data) > 0:\n",
    "            axes[i].hist(data, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "            axes[i].set_title(f'{feature}\\n(n={len(data):,})')\n",
    "            axes[i].set_xlabel('Value')\n",
    "            axes[i].set_ylabel('Frequency')\n",
    "            \n",
    "            # Add statistics\n",
    "            mean_val = data.mean()\n",
    "            std_val = data.std()\n",
    "            axes[i].axvline(mean_val, color='red', linestyle='--', alpha=0.7, label=f'Mean: {mean_val:.2f}')\n",
    "            axes[i].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "if len(numeric_cols) > 0:\n",
    "    print(\"üîó Correlation Analysis:\")\n",
    "    \n",
    "    # Select subset for correlation (too many features can be slow)\n",
    "    correlation_features = top_features[:10]  # Top 10 features\n",
    "    corr_matrix = X[correlation_features].corr()\n",
    "    \n",
    "    # Plot correlation heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))  # Mask upper triangle\n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "                square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
    "    plt.title('Feature Correlation Matrix (Top 10 Features)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find highly correlated feature pairs\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            corr_val = corr_matrix.iloc[i, j]\n",
    "            if abs(corr_val) > 0.8:  # High correlation threshold\n",
    "                high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_val))\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        print(f\"\\n‚ö†Ô∏è  Highly correlated feature pairs (|r| > 0.8):\")\n",
    "        for feat1, feat2, corr_val in high_corr_pairs:\n",
    "            print(f\"   {feat1} <-> {feat2}: {corr_val:.3f}\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No highly correlated feature pairs found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Attack Type Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature distributions by attack type\n",
    "if label_col and len(numeric_cols) > 0:\n",
    "    print(\"üéØ Feature Analysis by Attack Type:\")\n",
    "    \n",
    "    # Select a few key features for analysis\n",
    "    key_features = top_features[:6]  # Top 6 features\n",
    "    \n",
    "    # Create box plots for each feature by attack type\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, feature in enumerate(key_features):\n",
    "        # Prepare data (handle infinite values)\n",
    "        plot_data = df[[feature, label_col]].copy()\n",
    "        plot_data[feature] = plot_data[feature].replace([np.inf, -np.inf], np.nan)\n",
    "        plot_data = plot_data.dropna()\n",
    "        \n",
    "        if len(plot_data) > 0:\n",
    "            # Create box plot\n",
    "            sns.boxplot(data=plot_data, x=label_col, y=feature, ax=axes[i])\n",
    "            axes[i].set_title(f'{feature} by Attack Type')\n",
    "            axes[i].tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # Limit y-axis for better visualization (remove extreme outliers)\n",
    "            q1 = plot_data[feature].quantile(0.25)\n",
    "            q3 = plot_data[feature].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "            axes[i].set_ylim(lower_bound, upper_bound)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical comparison between normal and attack traffic\n",
    "if label_col:\n",
    "    print(\"üìä Normal vs Attack Traffic Comparison:\")\n",
    "    \n",
    "    # Separate normal and attack traffic\n",
    "    normal_traffic = df[df[label_col] == 'BENIGN'] if 'BENIGN' in df[label_col].values else df[df[label_col].str.contains('BENIGN|Normal', case=False, na=False)]\n",
    "    attack_traffic = df[df[label_col] != 'BENIGN'] if 'BENIGN' in df[label_col].values else df[~df[label_col].str.contains('BENIGN|Normal', case=False, na=False)]\n",
    "    \n",
    "    print(f\"   Normal traffic samples: {len(normal_traffic):,}\")\n",
    "    print(f\"   Attack traffic samples: {len(attack_traffic):,}\")\n",
    "    \n",
    "    if len(normal_traffic) > 0 and len(attack_traffic) > 0:\n",
    "        # Compare key features\n",
    "        comparison_features = top_features[:5]  # Top 5 features\n",
    "        \n",
    "        comparison_stats = []\n",
    "        for feature in comparison_features:\n",
    "            normal_mean = normal_traffic[feature].mean()\n",
    "            attack_mean = attack_traffic[feature].mean()\n",
    "            normal_std = normal_traffic[feature].std()\n",
    "            attack_std = attack_traffic[feature].std()\n",
    "            \n",
    "            comparison_stats.append({\n",
    "                'Feature': feature,\n",
    "                'Normal_Mean': normal_mean,\n",
    "                'Attack_Mean': attack_mean,\n",
    "                'Normal_Std': normal_std,\n",
    "                'Attack_Std': attack_std,\n",
    "                'Mean_Ratio': attack_mean / normal_mean if normal_mean != 0 else np.inf\n",
    "            })\n",
    "        \n",
    "        comparison_df = pd.DataFrame(comparison_stats)\n",
    "        display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Preprocessing Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing recommendations\n",
    "print(\"üîß Data Preprocessing Recommendations:\")\n",
    "print(\"\\n1. üìä Feature Engineering:\")\n",
    "\n",
    "# Check for categorical features that might need encoding\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "if categorical_features:\n",
    "    print(f\"   - Encode categorical features: {categorical_features}\")\n",
    "else:\n",
    "    print(\"   ‚úÖ No categorical features found\")\n",
    "\n",
    "# Check for features with high cardinality\n",
    "high_cardinality_features = []\n",
    "for col in categorical_features:\n",
    "    unique_count = X[col].nunique()\n",
    "    if unique_count > 50:  # High cardinality threshold\n",
    "        high_cardinality_features.append((col, unique_count))\n",
    "\n",
    "if high_cardinality_features:\n",
    "    print(f\"   - High cardinality features (consider target encoding):\")\n",
    "    for col, count in high_cardinality_features:\n",
    "        print(f\"     * {col}: {count} unique values\")\n",
    "\n",
    "print(\"\\n2. üßπ Data Cleaning:\")\n",
    "if inf_counts:\n",
    "    print(f\"   - Handle infinite values in: {list(inf_counts.keys())}\")\n",
    "if len(missing_features) > 0:\n",
    "    print(f\"   - Handle missing values in: {missing_features.index.tolist()}\")\n",
    "if zero_var_features:\n",
    "    print(f\"   - Remove zero variance features: {len(zero_var_features)} features\")\n",
    "\n",
    "print(\"\\n3. ‚öñÔ∏è Class Imbalance:\")\n",
    "if label_col:\n",
    "    class_imbalance_ratio = class_counts.max() / class_counts.min()\n",
    "    print(f\"   - Imbalance ratio: {class_imbalance_ratio:.2f}:1\")\n",
    "    if class_imbalance_ratio > 10:\n",
    "        print(\"   - Consider: SMOTE, class weights, or stratified sampling\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ Relatively balanced dataset\")\n",
    "\n",
    "print(\"\\n4. üî§ Tokenization Strategy for BERT:\")\n",
    "print(\"   - Convert numeric features to text representation\")\n",
    "print(\"   - Consider binning continuous features\")\n",
    "print(\"   - Create flow-based sequences\")\n",
    "print(\"   - Normalize feature values before tokenization\")\n",
    "\n",
    "print(\"\\n5. üìè Scaling:\")\n",
    "print(\"   - StandardScaler for features with normal distribution\")\n",
    "print(\"   - RobustScaler for features with outliers\")\n",
    "print(\"   - MinMaxScaler for bounded features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Processed Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a processed sample for further analysis\n",
    "if df is not None:\n",
    "    print(\"üíæ Saving processed data sample...\")\n",
    "    \n",
    "    # Create a clean sample\n",
    "    sample_df = df.copy()\n",
    "    \n",
    "    # Basic cleaning\n",
    "    for col in numeric_cols:\n",
    "        # Replace infinite values with NaN\n",
    "        sample_df[col] = sample_df[col].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Save sample\n",
    "    sample_path = PROCESSED_DIR / 'cicids2017_sample.csv'\n",
    "    sample_df.to_csv(sample_path, index=False)\n",
    "    print(f\"‚úÖ Saved sample to: {sample_path}\")\n",
    "    \n",
    "    # Save feature information\n",
    "    feature_info = {\n",
    "        'total_features': len(feature_cols),\n",
    "        'numeric_features': len(numeric_cols),\n",
    "        'categorical_features': len(categorical_features),\n",
    "        'top_features': top_features,\n",
    "        'zero_variance_features': zero_var_features,\n",
    "        'high_correlation_pairs': high_corr_pairs,\n",
    "        'class_distribution': class_counts.to_dict() if label_col else None\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    info_path = PROCESSED_DIR / 'feature_analysis.json'\n",
    "    with open(info_path, 'w') as f:\n",
    "        json.dump(feature_info, f, indent=2, default=str)\n",
    "    print(f\"‚úÖ Saved feature analysis to: {info_path}\")\n",
    "\n",
    "print(\"\\nüéâ Data exploration completed!\")\n",
    "print(\"\\nüìã Next Steps:\")\n",
    "print(\"   1. Download full CICIDS2017 dataset\")\n",
    "print(\"   2. Implement data preprocessing pipeline\")\n",
    "print(\"   3. Develop tokenization strategy for BERT\")\n",
    "print(\"   4. Train baseline models\")\n",
    "print(\"   5. Implement BERT-IDS architecture\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert-ids",
   "language": "python",
   "name": "bert-ids"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}